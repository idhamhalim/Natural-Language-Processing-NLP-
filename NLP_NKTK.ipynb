{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "\n",
    "- Natural Language Toolkit (NLTK)\n",
    "- Text Preprocessing\n",
    "    - Noise Removal\n",
    "        - Language stopwords \n",
    "        - URLs or links\n",
    "        - Social media entities \n",
    "        - Punctuations \n",
    "    - Lexicon Normalization\n",
    "        - Stemming\n",
    "        - Lemmatization\n",
    "- Part of Speech Tagging\n",
    "- Phrase Detection\n",
    "    - chunking\n",
    "    - chinking \n",
    "- Some useful functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit (NLTK) module with Python\n",
    "\n",
    "The NLTK module is a massive tool kit, aimed at helping you with the entire Natural Language Processing (NLP) methodology. It is an open source library in python. NLTK will aid you with everything from splitting sentences from paragraphs, splitting up words, recognizing the part of speech of those words, highlighting the main subjects, and then even with helping your machine to understand what the text is all about\n",
    "\n",
    "#### Advantages of NLTK\n",
    "- Has support for most NLP tasks\n",
    "- Provide acsses to numerous text corpora\n",
    "\n",
    "\n",
    "Let to explain some terms:\n",
    "- Tokenization – process of converting a text into tokens\n",
    "- Tokens – words or sentences or entities present in the text\n",
    "- Lexicon – Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons.\n",
    "- Corpus – Body of text, singular. Corpora is the plural of this. Example: A collection of medical journals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda5\\lib\\site-packages\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda5\\lib\\site-packages (from nltk)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk  \n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Since, text is the most unstructured form of all the available data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing.\n",
    "\n",
    "It is comprised of three steps:\n",
    "\n",
    "- <b>Noise Removal</b>\n",
    "    - Language stopwords (commonly used words of a language – is, am, the, of, in etc) \n",
    "    - URLs or links\n",
    "    - Social media entities (mentions, hashtags)\n",
    "    - Punctuations \n",
    "    - Industry specific words \n",
    "<BR> <BR>    \n",
    "- <b>Lexicon Normalization</b>\n",
    "     - Stemming\n",
    "     - Lemmatization\n",
    "\n",
    "let's show an example of how one might actually tokenize something into tokens with the NLTK module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We are attacking on their left flank but are losing many men.', 'We cannot see the enemy army.', 'Nothing else to report.', 'We are ready to attack but are waiting for your orders.']\n",
      "['We', 'are', 'attacking', 'on', 'their', 'left', 'flank', 'but', 'are', 'losing', 'many', 'men', '.', 'We', 'can', 'not', 'see', 'the', 'enemy', 'army', '.', 'Nothing', 'else', 'to', 'report', '.', 'We', 'are', 'ready', 'to', 'attack', 'but', 'are', 'waiting', 'for', 'your', 'orders', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#example_sent = \"Paragraphs can contain many different kinds of information. A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects. Regardless of the kind of information they contain, all paragraphs share certain characteristics. One of the most important of these is a topic sentence.\"\n",
    "example_sent =\"We are attacking on their left flank but are losing many men. We cannot see the enemy army. Nothing else to report. We are ready to attack but are waiting for your orders.\"\n",
    "#example_sent =\"Here are some very simple basic sentences. They won't be very interesting, I'm afraid.\", The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"\n",
    "\n",
    "sen_tokens = sent_tokenize(example_sent)\n",
    "print(sen_tokens) #sentence tokenization #each sentence is a token\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "print(word_tokens) #word tokenization #each word is a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "\"They won't be very interesting, I'm afraid.\",\n",
    "\"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs] #using for loop to tokenize corpora(plural of corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Removal\n",
    "\n",
    "Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n",
    "\n",
    "This step deals with removal of all types of noisy entities present in the text.\n",
    "\n",
    "A general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english')) #assign language that we want to use for stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'attacking', 'left', 'flank', 'losing', 'many', 'men', '.', 'We', 'see', 'enemy', 'army', '.', 'Nothing', 'else', 'report', '.', 'We', 'ready', 'attack', 'waiting', 'orders', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the stopwprds from the word's tocenized text\n",
    "words_filtered = [w for w in word_tokens if not w in stop_words]\n",
    "print(words_filtered)\n",
    "len(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'attacking', 'left', 'flank', 'losing', 'many', 'men', 'we', 'see', 'enemy', 'army', 'nothing', 'else', 'report', 'we', 'ready', 'attack', 'waiting', 'orders']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all punctuation \n",
    "words_filtered2 = [w.lower() for w in words_filtered if w.isalpha()]\n",
    "print(words_filtered2)\n",
    "len(words_filtered2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove URLs or links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regular Expresion\n",
    "\n",
    "Regular expression is a sequence of character(s) mainly used to find and replace patterns in a string or file. They are supported by most of the programming languages like python, perl, R, Java and many others.\n",
    "\n",
    "Regular expressions (<a href=\"https://docs.python.org/3/library/re.html\" target=\"_blank\" rel=\"noopener nofollow\">Regular expressions in Python 3</a>) use two types of characters:\n",
    "\n",
    "- <font color=green><b>a) Meta characters</b></font>: these characters have a special meaning. Here’s a complete list of them:\n",
    "<font color=red><b>. ^ $ * + ? { } [ ] | ( ) \\ </b></font>\n",
    "    - <b>Character matches:</b>\n",
    "    \n",
    "        <font color=red><b>.</b></font> :       Matches with any single character except newline\n",
    "        \n",
    "        <font color=red><b>^</b></font> :       Match the start of the string\n",
    "        \n",
    "      <font color=red><b>$</b></font> :       Match the end of the string\n",
    "      \n",
    "      <font color=red><b>[ ]</b></font> :    Matches any single character in a square bracket\n",
    "      \n",
    "      <b>[a-z]</b> :    Matches one of the range of character a,b,...,z\n",
    "      \n",
    "      <b>[^abc]</b> : Matches a character that is not in a,b or c\n",
    "      \n",
    "      <b>a<font color=red>|</font>b</b> :     Matches either a or b, where a and b are string\n",
    "      \n",
    "      <font color=red><b>( )</b></font> :     Groups regular expressions and returns matched text\n",
    "      \n",
    "      <font color=red><b>\\</b></font> :       It is used for special meaning characters\n",
    "    <br><br>\n",
    "    - <b> Characters symbols</b>\n",
    "    \n",
    "        <font color=red><b> \\b </b></font>: Matches word boundary\n",
    "        \n",
    "        <font color=red><b> \\d </b></font>: Any digit, equivalent to [0-9]\n",
    "        \n",
    "        <font color=red><b> \\D </b></font>: Any non-digit, equivalent to [^ 0-9]\n",
    "        \n",
    "        <font color=red><b> \\s </b></font>: Any whitespace, equivalent to [\\t\\n\\r\\f\\v]\n",
    "        \n",
    "        <font color=red><b> \\S </b></font>: Any non-whitespace, equivalent to [^ \\t\\n\\r\\f\\v]\n",
    "        \n",
    "        <font color=red><b> \\w </b></font>: Alphanumeric character, equivalent to [a-zA-z0-9_ ]\n",
    "        \n",
    "        <font color=red><b> \\W </b></font>: Non-alphanumeric character, equivalent to [^ a-zA-z0-9_ ]\n",
    "<br><br>\n",
    "    - <b> Repetitions:</b>\n",
    "\n",
    "        <font color=red><b>*</b></font> :       0 or more occurrences\n",
    "        \n",
    "        <font color=red><b>+</b></font> :       1 or more occurrences \n",
    "          \n",
    "        <font color=red><b>?</b></font> :       0 or 1 occurrence\n",
    "        \n",
    "        <font color=red><b>{</font><b>n<font color=red>}</b></font> :       Exactly n repetitions, n>=0\n",
    "        \n",
    "        <font color=red><b>{</font><b>n, <font color=red>}</b></font> :       At least n repetitions\n",
    "        \n",
    "        <font color=red><b>{</font><b> ,n<font color=red>}</b></font> :       At most n repetitions\n",
    "    \n",
    "\n",
    "- <font color=green><b>b) Literals </b></font>(like a,b,1,2…)\n",
    "\n",
    "In Python, we have module “re” that helps with regular expressions. So you need to import library re before you can use regular expressions in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re #module for regular expresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common uses of regular expressions are:\n",
    "- <font color=blue>Search a string </font>(search and match)\n",
    "- <font color=blue>Finding a string </font>(findall)\n",
    "- <font color=blue>Break string into a sub strings</font> (split)\n",
    "- <font color=blue>Replace part of a string </font>(sub)\n",
    "\n",
    "The most commonly used methods which The ‘re’ package provides to perform queries on an input string:\n",
    "\n",
    "- <font color=green><b>re.match(pattern, string)</b></font>: \n",
    "This method finds match if it occurs at start of the string.\n",
    "\n",
    "- <font color=green><b>re.search(pattern, string)</b></font>: \n",
    "It is similar to match() but it doesn’t restrict us to find matches at the beginning of the string only.\n",
    "search() method is able to find a pattern from any position of the string but it only returns the first occurrence of the search pattern.\n",
    "\n",
    "- <font color=green><b>re.findall (pattern, string)</b></font>: \n",
    "This method helps to get a list of all matching patterns. It has no constraints of searching from start or end.\n",
    "\n",
    "- <font color=green><b>re.split(pattern, string, maxsplit=0)</b></font>: \n",
    "This methods helps to split string by the occurrences of given pattern.\n",
    "\n",
    "- <font color=green><b>re.sub(pattern, replace, string)</b></font>:\n",
    "It helps to search a pattern and replace with a new sub string. If the pattern is not found, string is returned unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 2), match='AV'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = re.match(r'AV', 'AV Analytics Vidhya AV') #find how AV in string\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='voo'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#match() #find string \n",
    "value=\"voorheesville\"\n",
    "m = re.match(r\"voo\", value)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='voo'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#match()\n",
    "value=\"voorheesville\"\n",
    "m = re.match(r\"vo*\", value) # * - to check for occurence\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(3, 12), match='Analytics'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#search() # can find string in any position in a sentence\n",
    "result = re.search(r'Analytics', 'AV Analytics Vidhya AV')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AV', 'AV']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#findall()\n",
    "result = re.findall(r'AV', 'AV Analytics Vidhya AV')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Analytics Vidhya AV']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#findall()\n",
    "result = re.findall(r'(An.*)', 'AV Analytics Vidhya AV')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "- Find all words starting which start with 'd' or 'p' in the text bellow.\n",
    "\n",
    "   text=\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part',\n",
       " 'address',\n",
       " 'intermediate',\n",
       " 'senders',\n",
       " 'intermediate',\n",
       " 'multiple',\n",
       " 'addresses',\n",
       " 'administrator',\n",
       " 'address',\n",
       " 'distribution']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "text=\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\"\n",
    "result = re.findall(r'\\w*[dp]\\w+', text)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anal', 'tics']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#split()\n",
    "result=re.split(r'y','Analytics')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AV Anal', 'tics Vidh', 'a AV']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split()\n",
    "result=re.split(r'y','AV Analytics Vidhya AV')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AV Anal', 'tics Vidhya AV']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split()\n",
    "result=re.split(r'y','AV Analytics Vidhya AV',maxsplit=1) #maximum split for string\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '1', '2', '3']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split()\n",
    "# Separate on one or more non-digit characters in following text.\n",
    "\n",
    "value = \"one 1 two 2 three 3\"\n",
    "\n",
    "result=re.split(r'\\D+', value ) #maximum split for string\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ethics', 'Ethical']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split()\n",
    "# finding all words which include 'E' in bellow tweet \n",
    "\n",
    "tweet2 = '@UN @UN_Women \"Ethics are built right into the ideals and objectives of the United Nations\" \\\n",
    "#UNSG @ NY Society for Ethical Culture bit.ly/2guVelr'\n",
    "result = re.findall(r'\\w*[E]\\w+', tweet2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-43-aa95744fc6b3>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-43-aa95744fc6b3>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    text2 = w for w in text1 if re.findall(r'\\w*[E]\\w+',w)]\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text1=re.split('', tweet2 ) #maximum split for string\n",
    "\n",
    "text2 = w for w in text1 if re.findall(r'\\w*[E]\\w+',w)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AV is largest Analytics community of the World'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sub()\n",
    "result=re.sub(r'India','the World','AV is largest Analytics community of India')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two useful methods\n",
    "\n",
    "- <font color=green><b>join()</b></font>: The method returns a string in which the string elements of sequence have been joined by str separator.\n",
    "\n",
    "- <font color=green><b>strip()</b></font>: The method returns a copy of the string in which all chars have been stripped from the beginning and the end of the string (default whitespace characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AV', 'is', 'largest', 'Analytics', 'community', 'of', 'India']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AV is largest Analytics community of India'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join() \n",
    "tweet2 = 'AV is largest Analytics community of India'\n",
    "text1 = tweet2.split(' ')\n",
    "print(text1)\n",
    "text3= ' '.join(text1)\n",
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick brown  fox  jumped over the lazy dog.\n",
      "['A', 'quick', 'brown', '', 'fox', '', 'jumped', 'over', 'the', 'lazy', 'dog.']\n"
     ]
    }
   ],
   "source": [
    "# strip()  \n",
    "text8 ='   A quick brown  fox  jumped over the lazy dog.  \\n '\n",
    "text9=text8.strip() \n",
    "\n",
    "print(text9)\n",
    "\n",
    "print(re.split(' ',text9))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a tweet with a url: and there is no any url'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove URLs or links\n",
    "\n",
    "tweet = 'This is a tweet with a url:http://t.co/0DlGChTBIx and there is no any url'\n",
    "tweet_clean = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "tweet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: 123\n",
      "  END: 123\n",
      "\n",
      "\n",
      "START: 4cat\n",
      "\n",
      "\n",
      "  END: dog5\n",
      "\n",
      "\n",
      "START: 6mouse\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# what do this code?\n",
    "list = [\"123\", \"4cat\", \"dog5\", \"6mouse\", \"mouse\"]\n",
    "for w in list:\n",
    "\n",
    "    m = re.match(\"^\\d\", w)\n",
    "    if m:\n",
    "        print(\"START:\", w)\n",
    "\n",
    "    m = re.match(\".*\\d$\", w)\n",
    "    if m:\n",
    "        print(\"  END:\", w) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Write a code to extract:\n",
    "- 1) hashtags from following tweet.\n",
    "\n",
    "    tweet1 = \"@nltk Text analysis is awesome! #regex #pandas #python\"\n",
    "<br><br>    \n",
    "    \n",
    "- 2) callouts from given tweet.\n",
    "\n",
    "    tweet2 = '@UN @UN_Women \"Ethics are built right into the ideals and objectives of the United Nations\"\n",
    "<br><br>      \n",
    "- 3) the words which start with 'vi' without using ^ from the text bellow.\n",
    "\n",
    "    text = ' visit123 \"Ethics are21 view right into21 the via ideals and objectives of the United Nations\" \\ #UNSG @21 NY23 Society for134  a14 Ethical43 view23 vital'\n",
    " <br><br>     \n",
    "- 4) all words exept those including 'dog' from given text.\n",
    "\n",
    "    text = '100cat 223cat 534dog 400cat 500car 345dog 847bar'\n",
    "<br><br>\n",
    "- 5) Email address from following text.\n",
    "\n",
    "    text='John.Smith@example.com Ethics are built right into the ideals and objectives of the United Nations\" \\ #UNSG @21 NY Society for Ethical Culture local-part@domain.org'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#regex', '#pandas', '#python']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here - Solution 1\n",
    "tweet1 = \"@nltk Text analysis is awesome! #regex #pandas #python\"\n",
    "result1 = re.findall(r'\\w*[#]\\w+', tweet1)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-46-68936e837396>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-46-68936e837396>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    tweet2 = \"@UN @UN_Women \"Ethics are built right into the ideals and objectives of the United Nations\"\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# your code here - Solution 2\n",
    "tweet2 = '@UN @UN_Women \"Ethics are built right into the ideals and objectives of the United Nations\" \n",
    "result2 = re.findall(r'\\w*[@]\\w+', tweet2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here - Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here - Solution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here - Solution 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon Normalization\n",
    "\n",
    "Another type of textual noise is about the multiple representations exhibited by single word.\n",
    "\n",
    "For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”, Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form (also known as lemma).\n",
    "\n",
    "The most common lexicon normalization practices are :\n",
    "\n",
    "- <b> Stemming</b>:\n",
    "    - a primary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word\n",
    "    \n",
    "    \n",
    "- <b>Lemmatization</b>: \n",
    "    - an organized & step by step procedure of obtaining the root form of the word \n",
    "    - it makes use of: \n",
    "        - vocabulary (dictionary importance of words) \n",
    "        - morphological analysis (word structure and grammar relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "littl\n",
      "yellow\n",
      "dog\n",
      "bark\n",
      "at\n",
      "the\n",
      "cat\n",
      ".\n",
      "think\n",
      ",\n",
      "play\n",
      ",\n",
      "cat\n",
      ",\n",
      "leav\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "new_text=\"the little yellow dog barked at the cat. thinking,plays,cats, leaves\"\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "    print(ps.stem(w)) # find root of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n",
      "eventuel\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\", 'eventuellement', 'pythonic']\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "goose\n",
      "rock\n",
      "life\n",
      "leaf\n",
      "worse\n",
      "running\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words =(\"cats\",\"geese\",\"rocks\",\"lives\",\"leaves\",\"worse\",\"running\")\n",
    "for w in words:\n",
    "    print(lemmatizer.lemmatize(w)) #can find root of word #works with nouns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "best\n",
      "bad\n",
      "running\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"worse\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"running\"))\n",
    "print(lemmatizer.lemmatize(\"running\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part of speech tagging\n",
    "\n",
    "Apart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. Recall from your high school grammer that part of speech are these verb classes like nouns, verbs, adjectives, adverbs etc.\n",
    "\n",
    "Here is a <b>list of pos-tags</b>, what they mean, and some examples:(<font color=blue>Tag</font>, Word class, <font color=green>Example</font>)\n",
    "\n",
    "<font color=blue>CC</font>\tcoordinating conjunction\n",
    "<font color=blue>CD</font>\tcardinal digit\n",
    "<font color=blue>DT</font>\tdeterminer\n",
    "<font color=blue>EX\t</font>existential there (like: <font color=green>\"there is\"</font> ... think of it like <font color=green>\"there exists\"</font>)\n",
    "<font color=blue>FW\t</font>foreign word\n",
    "<font color=blue>IN\t</font>preposition/subordinating conjunction\n",
    "<font color=blue>JJ\t</font>adjective\t<font color=green>'big'</font>\n",
    "<font color=blue>JJR\t</font>adjective, comparative\t<font color=green>'bigger'</font>\n",
    "<font color=blue>JJS\t</font>adjective, superlative\t<font color=green>'biggest'</font>\n",
    "<font color=blue>LS\t</font>list marker\t<font color=green>1)</font>\n",
    "<font color=blue>MD\t</font>modal\t<font color=green>could, will</font>\n",
    "<font color=blue>NN\t</font>noun, singular <font color=green>'desk'</font>\n",
    "<font color=blue>NNS\t</font>noun plural\t<font color=green>'desks'</font>\n",
    "<font color=blue>NNP\t</font>proper noun, singular\t<font color=green>'Harrison'</font>\n",
    "<font color=blue>NNPS\t</font>proper noun, plural\t<font color=green>'Americans'</font>\n",
    "<font color=blue>PDT\t</font>predeterminer\t<font color=green>'all the kids'</font>\n",
    "<font color=blue>POS\t</font>possessive ending\tparent<font color=green>'s</font>\n",
    "<font color=blue>PRP\t</font>personal pronoun\t<font color=green>I, he, she</font>\n",
    "<font color=blue>PRPS\t</font>possessive pronoun\t<font color=green>my, his, hers</font>\n",
    "<font color=blue> RB\t</font>adverb\t<font color=green>very, silently</font>\n",
    "<font color=blue> RBR</font>\tadverb, comparative\t<font color=green>better</font>\n",
    "<font color=blue>RBS</font>\tadverb, superlative\t<font color=green>best</font>\n",
    "<font color=blue>RP\t</font>particle<font color=green>\tgive up</font>\n",
    "<font color=blue>TO\t</font>to\tgo <font color=green>'to' </font>the store.\n",
    "<font color=blue>UH\t</font>interjection\t<font color=green>errrrrrrrm</font>\n",
    "<font color=blue>VB\t</font>verb, base form\t<font color=green>take</font>\n",
    "<font color=blue>VBD\t</font>verb, past tense\t<font color=green>took</font>\n",
    "<font color=blue>VBG</font>\tverb, gerund/present participle\t<font color=green>taking</font>\n",
    "<font color=blue>VBN\t</font>verb, past participle\t<font color=green>taken</font>\n",
    "<font color=blue>VBP\t</font>verb, sing. present, non-3d\t<font color=green>take</font>\n",
    "<font color=blue>VBZ\t</font>verb, 3rd person sing. present\t<font color=green>takes</font>\n",
    "<font color=blue>WDT\t</font>wh-determiner\t<font color=green>which</font>\n",
    "<font color=blue>WP\t</font>wh-pronoun\t<font color=green>who, what</font>\n",
    "<font color=blue>WPS</font>possessive wh-pronoun\t<font color=green>whose</font>\n",
    "<font color=blue> WRB\t</font>wh-abverb\t<font color=green>where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cat', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Part of speech tagging\n",
    "\n",
    "sample_text=\"the little yellow dog barked at the cat\"\n",
    "words = word_tokenize(sample_text)\n",
    "\n",
    "for i in words:\n",
    "    tagged = nltk.pos_tag(words)\n",
    "print(tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Phrase Detection\n",
    "\n",
    "Phrase Detection is finding phrases in general from a corpus. One way for Phrase Detection is used to apply 'chunking' and 'chinking'.\n",
    "\n",
    "<b>chunking</b>: \n",
    "   - Group words into hopefully meaningful chunks. \n",
    "   - One of the main goals of chunking is to group into what are known as \"noun phrases.\"\n",
    "   - These are phrases of one or more words that contain:\n",
    "        - a noun \n",
    "        - maybe some descriptive words\n",
    "        - maybe a verb \n",
    "        - maybe something like an adverb \n",
    "        \n",
    "   - The rules that make up a chunk grammar use <b>tag patterns</b> to describe sequences of tagged words. \n",
    "       -  Tag pattern is a sequence of part-of-speech tags delimited using angle brackets\n",
    "\n",
    " In order to chunk, we combine  <b>the part of speech tags </b> with <b> regular expressions </b>.\n",
    "\n",
    "<b>chinking</b>:\n",
    "   - the process of removing a sequence of tokens from a chunk\n",
    "   - denote the chink, after the chunk, with }{ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (Chunk the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "#chunking\n",
    "import nltk\n",
    "sample_text=\"the little yellow dog barked at the cat\"\n",
    "words = word_tokenize(sample_text)\n",
    "\n",
    "for i in words:\n",
    "    tagged = nltk.pos_tag(words)\n",
    "chunkGram = \"Chunk: {<DT>?<JJ>*<NN>}\"\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tagged)\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Rapunzel/NNP)\n",
      "  let/VBD\n",
      "  down/RP\n",
      "  her/PRP$\n",
      "  (NP long/JJ golden/JJ hair/NN))\n"
     ]
    }
   ],
   "source": [
    "#chunking\n",
    "sample_text=\"Rapunzel let down her long golden hair\"\n",
    "words = word_tokenize(sample_text)\n",
    "\n",
    "for i in words:\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    \n",
    "chunkGram = r\"\"\"\n",
    "  NP: {<DT>?<JJ>*<NN>} \n",
    "      {<NNP>}\"\"\"\n",
    "#chunk can use more than one rule   \n",
    "  \n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tagged)\n",
    "print(chunked)\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#chinking\n",
    "sample_text=\"the little yellow dog barked at the cat\"\n",
    "words = word_tokenize(sample_text)\n",
    "\n",
    "for i in words:\n",
    "    tagged = nltk.pos_tag(words)\n",
    "chunkGram = r\"\"\"Chunk: {<DT>?<JJ>*<NN>}\n",
    "                        }<DT>{ \"\"\"\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tagged)\n",
    "print(chunked)\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#chinking\n",
    "sample_text=\"the little yellow dog barked at the cat\"\n",
    "words = word_tokenize(sample_text)\n",
    "\n",
    "for i in words:\n",
    "    tagged = nltk.pos_tag(words)\n",
    "chunkGram = r\"\"\"Chunk: {<.*>+}  \n",
    "                        }<VB.?|IN|DT|TO>+{ \"\"\"\n",
    "#every word is accepted into chunk\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tagged)\n",
    "print(chunked)\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Some useful functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding specific words\n",
    "- long words:words that are most than 3 letters long\n",
    "- Capitalized words:the words that start by capital letter a-z\n",
    "- Words that end / start with specific letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1 =\"We are attacking on their left flank but are losing many men. We cannot see the enemy army. Nothing else to report. We are ready to attack but are waiting for your orders.\"\n",
    "text2 = word_tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#long words\n",
    "text3 = [w for w in text2 if len(w)>3]\n",
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Capitalized words\n",
    "[w for w in text2 if w.istitle()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Words that end with e\n",
    "[w for w in text2 if w.endswith('e')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Words that start with t\n",
    "[w for w in text2 if w.startswith('t')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding unique words: Using set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding unique words\n",
    "text3=\"To be or not to be\"\n",
    "text4=text3.split(' ')\n",
    "print(len(text4))\n",
    "print(set(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(set([w.lower() for w in text4])) #before finding unique word, good idea to set everythin to lower case\n",
    "len(set([w.lower() for w in text4]))   # because set detects lower & upper caase letters to be unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist # find freq distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text=\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text=\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\"\n",
    "text_tokens=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_tokens1=[w.lower() for w in text_tokens]\n",
    "\n",
    "dist = FreqDist(text_tokens1)\n",
    "len(dist)\n",
    "#dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist['for'] #how many times for appear in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "- Find words have length at least 3 and occur at least 3 times in previous text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "findwords = [w for w in vocab if len(w)>2 and dist[w]>=3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
