{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Contents:\n",
    "\n",
    "- 1) Introduction to Natural Language Processing\n",
    "- 2) Sentiment Analysis\n",
    "    - Model Selection in scikit-learn\n",
    "    - Extracting features\n",
    "        - Bag-of-words\n",
    "    - Logistic Regression classification\n",
    "    - Tfidf\n",
    "    - N-gram\n",
    "- 3) Text Classification\n",
    "    - Using sklearn's NaiveBayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Natural Language Processing\n",
    "NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as – automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.\n",
    "\n",
    "What better way than to use a popular use case application: Amazon review sentiment analysis, to better understand how text information can be parsed and processed into something useful for ML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Case Study: Sentiment Analysis\n",
    "\n",
    "We will be working on a large dataset of reviews of unlocked mobile phones sold on Amazon.com that has been collected by Crawlers et al. in December, 2016. The Amazon reviews dataset consists of 400 thousand reviews to find out insights with respect to reviews, ratings, price and their relationships.\n",
    "\n",
    "#### Dataset Content \n",
    "\n",
    "Given below are the fields:\n",
    "\n",
    "- Product Title\n",
    "- Brand\n",
    "- Price\n",
    "- Rating\n",
    "- Review text\n",
    "- Number of people who found the review helpful\n",
    "\n",
    "Our main end goal here is to learn how to extract meaningful information from a subset of these reviews to build a machine learning model that can predict whether a certain reviewer liked or disliked a mobile phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = pd.read_csv('Amazon_Unlocked_Mobile.csv', encoding=\"utf8\")\n",
    "\n",
    "# shuffle rows of dataframe\n",
    "df = df.sample(frac=1, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove any 'neutral' ratings equal to 3\n",
    "df = df[df['Rating'] != 3]\n",
    "\n",
    "# Encode 4s and 5s as 1 (rated positively)\n",
    "# Encode 1s and 2s as 0 (rated poorly)\n",
    "df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], \n",
    "                                                    df['Positively Rated'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I own 2 of these, one is international version that I used during travels and another is T-Mobile's.T-Mobile's came with Android 2.3.5 and international one with 2.3.6, I liked 2.3.6 more, less glitchesand runs smoother,also battery efficient. Anyway,T-mobile applies a software update pretty right awayand now both run the same version. International has faster cpu (1.5Ghz) vs T-Mobile's 1Ghz, thoughhonestly I don't feel difference. I don't run games and use the usual set of applications - Google'sGmail,Maps,Talk, Facebook's client and messenger, Skype (which is not officially supported on SamsungExhibit / Wonder but works great) and few other social networking / informational apps. T-Mobile activationwas a blaze.What I like: screen is nice except in sunny places, surprisingly clear for TFT however my eyes like AMOLEDmore. Size is great if you can live with smaller screen. CPU speed is superb for both 1Ghz and 1.5Ghz. Battery lifeis fantastic.What I don't like: TFT screen,don't see anything in particular conditions (direct sun etc). Small size means lesscomfortable keyboard.Overall very nice smartphone for its price if you're ready to give up on screen size and quality.\n"
     ]
    }
   ],
   "source": [
    "print( X_train.iloc[10])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from text files\n",
    "\n",
    "\n",
    "Text files are actually series of words (ordered). In order to run machine learning algorithms we need to convert the text files into numerical feature vectors. We will be using bag of words model.\n",
    "\n",
    "## Bag-of-words (BOW)\n",
    "BOW model allows us to represent text as numerical feature vectors. The idea behind BOW is quite simple and can be summarized as follows:\n",
    "- 1) Create a vocabulary of unique tokens (or words) from the entire set \n",
    "    of documents.\n",
    "- 2) Construct a feature vector from each document that contains the counts of how often each word occurs in the particular document.\n",
    "\n",
    "Since the unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will consist of mostly zeros, which is why we call them sparse. For this reason we say that bags of words are typically <b>high-dimensional sparse datasets</b>.\n",
    "\n",
    "{for our example. Briefly, we segment each text file into words (for English splitting by space), and count # of times each word occurs in each document and finally assign each word an integer id. Each unique word in our dictionary will correspond to a feature (descriptive feature).}\n",
    "\n",
    "\n",
    "### Transform words into vectors (CountVectorizer)\n",
    "To construct a bag-of-words model based on the word counts in the respective documents, we can use the `CountVectorizer` class implemented in `scikit-learn`. As we will see in the following codes, the `CountVectorizer` class takes an array of text data, which can be documents or just sentences, and constructs the bag-of-words model for us:\n",
    "\n",
    "Scikit-learn has a high level component which will create feature vectors for us <b>‘CountVectorizer’</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'])\n",
    "\n",
    "# Fit the CountVectorizer to the training data \n",
    "vect1=CountVectorizer().fit(docs)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix. \n",
    "bag = vect1.transform(docs)\n",
    "\n",
    "#jadi matrix\n",
    "bag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0, 'is': 1, 'shining': 2, 'sun': 3, 'sweet': 4, 'the': 5, 'weather': 6}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'is', 'shining', 'sun', 'sweet', 'the', 'weather']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 15 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1) Do CountVectorizer for training data\n",
    "\n",
    "2) Detedrmine: \n",
    "- The number of features \n",
    "- The shape of sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231207, 53301)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data \n",
    "vect2=CountVectorizer().fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '00000',\n",
       " '000000',\n",
       " '0000000',\n",
       " '00000000000',\n",
       " '0000from',\n",
       " '0001',\n",
       " '0004',\n",
       " '000ma',\n",
       " '000mah',\n",
       " '000mh',\n",
       " '000restricted',\n",
       " '0051',\n",
       " '006',\n",
       " '007',\n",
       " '00am',\n",
       " '00bucks',\n",
       " '00for',\n",
       " '00it',\n",
       " '00k',\n",
       " '00now',\n",
       " '00pm',\n",
       " '00x2',\n",
       " '01',\n",
       " '011',\n",
       " '013435003182980',\n",
       " '014',\n",
       " '0155379',\n",
       " '016',\n",
       " '016s',\n",
       " '019s',\n",
       " '02',\n",
       " '02may13',\n",
       " '02mbps',\n",
       " '03',\n",
       " '032g',\n",
       " '0330',\n",
       " '03pm',\n",
       " '04',\n",
       " '0400',\n",
       " '044',\n",
       " '04pm',\n",
       " '04th',\n",
       " '04the',\n",
       " '05',\n",
       " '050',\n",
       " '0500tkx',\n",
       " '050mms',\n",
       " '050prot',\n",
       " '051',\n",
       " '056',\n",
       " '0572013',\n",
       " '0577454',\n",
       " '05788690',\n",
       " '05th',\n",
       " '05the',\n",
       " '05using',\n",
       " '06',\n",
       " '061',\n",
       " '062',\n",
       " '0630',\n",
       " '066',\n",
       " '06pm',\n",
       " '07',\n",
       " '0780',\n",
       " '07am',\n",
       " '07gb',\n",
       " '07nov2015',\n",
       " '08',\n",
       " '0804245',\n",
       " '0808',\n",
       " '0825',\n",
       " '0829',\n",
       " '087',\n",
       " '087581287',\n",
       " '08in',\n",
       " '08mms',\n",
       " '08this',\n",
       " '09',\n",
       " '0909853',\n",
       " '09on',\n",
       " '0_1439_7',\n",
       " '0_150511',\n",
       " '0_print_120716',\n",
       " '0_user_manual',\n",
       " '0a',\n",
       " '0also',\n",
       " '0b3tbzlidhq7dce1bv05qdefaota',\n",
       " '0bj7255rf1f1a1118w65',\n",
       " '0c',\n",
       " '0cant',\n",
       " '0cesqfjad',\n",
       " '0dislikes',\n",
       " '0expandable',\n",
       " '0f',\n",
       " '0ff',\n",
       " '0gb',\n",
       " '0ghz',\n",
       " '0gravity',\n",
       " '0great',\n",
       " '0hd',\n",
       " '0hone',\n",
       " '0i',\n",
       " '0ii',\n",
       " '0in',\n",
       " '0inch',\n",
       " '0inchtype',\n",
       " '0k',\n",
       " '0l',\n",
       " '0lte',\n",
       " '0mb',\n",
       " '0mcc',\n",
       " '0mp',\n",
       " '0mpram',\n",
       " '0mps87ysgrvp9gkhps5d',\n",
       " '0mpthis',\n",
       " '0n',\n",
       " '0r',\n",
       " '0s',\n",
       " '0social',\n",
       " '0stars',\n",
       " '0this',\n",
       " '0ui',\n",
       " '0v',\n",
       " '0well',\n",
       " '0x',\n",
       " '0x1',\n",
       " '0ç',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '1000000',\n",
       " '1000000000',\n",
       " '1000000000000000',\n",
       " '10000000x',\n",
       " '10000mah',\n",
       " '1000mah',\n",
       " '1000mhz',\n",
       " '1000s',\n",
       " '1000x',\n",
       " '1001',\n",
       " '1001multi',\n",
       " '1005',\n",
       " '10050',\n",
       " '100cm',\n",
       " '100gb',\n",
       " '100hours',\n",
       " '100k',\n",
       " '100mah',\n",
       " '100mb',\n",
       " '100mbps',\n",
       " '100megs',\n",
       " '100min',\n",
       " '100minutes',\n",
       " '100puntos',\n",
       " '100s',\n",
       " '100th',\n",
       " '100usd',\n",
       " '100v',\n",
       " '100voice',\n",
       " '100x',\n",
       " '101',\n",
       " '1012017',\n",
       " '1016',\n",
       " '102',\n",
       " '1020',\n",
       " '10211',\n",
       " '1022',\n",
       " '1024',\n",
       " '1024ghz',\n",
       " '1024x768',\n",
       " '103',\n",
       " '1030',\n",
       " '10327',\n",
       " '1037',\n",
       " '104',\n",
       " '10400mah',\n",
       " '10454',\n",
       " '1048576k',\n",
       " '105',\n",
       " '1053',\n",
       " '1058',\n",
       " '1059333',\n",
       " '105dbm',\n",
       " '105f',\n",
       " '106',\n",
       " '1060',\n",
       " '10604',\n",
       " '1067',\n",
       " '106gsm',\n",
       " '106miles',\n",
       " '107',\n",
       " '1072',\n",
       " '10728gb',\n",
       " '1073',\n",
       " '108',\n",
       " '1080',\n",
       " '1080hd',\n",
       " '1080i',\n",
       " '1080p',\n",
       " '1080pthe',\n",
       " '1080x1080',\n",
       " '1080x1920',\n",
       " '108dbm',\n",
       " '109',\n",
       " '1092',\n",
       " '1096',\n",
       " '1097',\n",
       " '109model',\n",
       " '10am',\n",
       " '10as',\n",
       " '10audio',\n",
       " '10battery',\n",
       " '10cents',\n",
       " '10d',\n",
       " '10days',\n",
       " '10degrees',\n",
       " '10design',\n",
       " '10disclaimer',\n",
       " '10f',\n",
       " '10ft',\n",
       " '10gb',\n",
       " '10gbs',\n",
       " '10good',\n",
       " '10gps',\n",
       " '10h',\n",
       " '10hours',\n",
       " '10hr',\n",
       " '10hrs',\n",
       " '10i',\n",
       " '10k',\n",
       " '10kms',\n",
       " '10mb',\n",
       " '10mbps',\n",
       " '10mbs',\n",
       " '10megs',\n",
       " '10mgs',\n",
       " '10min',\n",
       " '10mins',\n",
       " '10minute',\n",
       " '10minutes',\n",
       " '10mm',\n",
       " '10mp',\n",
       " '10mts',\n",
       " '10p',\n",
       " '10pm',\n",
       " '10point',\n",
       " '10processor',\n",
       " '10radio',\n",
       " '10s',\n",
       " '10screen',\n",
       " '10sec',\n",
       " '10size',\n",
       " '10so',\n",
       " '10sound',\n",
       " '10speed',\n",
       " '10th',\n",
       " '10the',\n",
       " '10this',\n",
       " '10times',\n",
       " '10touchscreen',\n",
       " '10txt',\n",
       " '10uscw52ena001558',\n",
       " '10w',\n",
       " '10x',\n",
       " '10x10mhz',\n",
       " '10xcons',\n",
       " '10year',\n",
       " '10yo',\n",
       " '10yr',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '11059mem',\n",
       " '1109miami',\n",
       " '110e',\n",
       " '110f',\n",
       " '110mb',\n",
       " '110v',\n",
       " '110w',\n",
       " '111',\n",
       " '11111',\n",
       " '1111111',\n",
       " '1111111111',\n",
       " '1113',\n",
       " '1116',\n",
       " '111display',\n",
       " '112',\n",
       " '1122',\n",
       " '11230',\n",
       " '11230doral',\n",
       " '113',\n",
       " '1130',\n",
       " '1135',\n",
       " '1136',\n",
       " '113mb',\n",
       " '113van',\n",
       " '114',\n",
       " '115',\n",
       " '1150',\n",
       " '1154',\n",
       " '115f',\n",
       " '116',\n",
       " '116mb',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '1196220',\n",
       " '119gb',\n",
       " '11ac',\n",
       " '11am',\n",
       " '11available',\n",
       " '11b',\n",
       " '11g',\n",
       " '11gb',\n",
       " '11gdespues',\n",
       " '11h00',\n",
       " '11i',\n",
       " '11nfeaturescon',\n",
       " '11pm',\n",
       " '11s',\n",
       " '11st',\n",
       " '11th',\n",
       " '11the',\n",
       " '11x',\n",
       " '11yeqr',\n",
       " '11yr',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '12000mah',\n",
       " '1200hz',\n",
       " '1200mhz',\n",
       " '1200x1600',\n",
       " '1208',\n",
       " '120fps',\n",
       " '120gbs',\n",
       " '120kbps',\n",
       " '120mb',\n",
       " '120mp',\n",
       " '120sec',\n",
       " '120usd',\n",
       " '120v',\n",
       " '120voltage',\n",
       " '121',\n",
       " '1217asus',\n",
       " '121switch',\n",
       " '1224',\n",
       " '123',\n",
       " '1232',\n",
       " '1234',\n",
       " '12345',\n",
       " '123456',\n",
       " '12393',\n",
       " '124',\n",
       " '124gb',\n",
       " '125',\n",
       " '1254',\n",
       " '12580',\n",
       " '126',\n",
       " '12600',\n",
       " '1260p',\n",
       " '127',\n",
       " '127mb',\n",
       " '128',\n",
       " '1280',\n",
       " '1280x720',\n",
       " '1280x720p',\n",
       " '1280x960',\n",
       " '128g',\n",
       " '128gb',\n",
       " '128gb32gb',\n",
       " '128gbbattery',\n",
       " '128gig',\n",
       " '128gigs',\n",
       " '128k',\n",
       " '128kbps',\n",
       " '128kps',\n",
       " '128mb',\n",
       " '128sd',\n",
       " '129',\n",
       " '12days',\n",
       " '12feb',\n",
       " '12games',\n",
       " '12gb',\n",
       " '12hours',\n",
       " '12hrs',\n",
       " '12hs',\n",
       " '12i',\n",
       " '12mbps',\n",
       " '12mm',\n",
       " '12mn',\n",
       " '12mp',\n",
       " '12mpixel',\n",
       " '12mpx',\n",
       " '12not',\n",
       " '12oz',\n",
       " '12pm',\n",
       " '12rd',\n",
       " '12th',\n",
       " '12x24',\n",
       " '12year',\n",
       " '12yr',\n",
       " '12yrs',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '1300ghz',\n",
       " '1308',\n",
       " '130mb',\n",
       " '130usd',\n",
       " '131',\n",
       " '13125051234would',\n",
       " '131region',\n",
       " '132',\n",
       " '1320',\n",
       " '133',\n",
       " '1334',\n",
       " '133781',\n",
       " '134599',\n",
       " '135',\n",
       " '1351960555',\n",
       " '136',\n",
       " '1365509935',\n",
       " '1366x768',\n",
       " '1369',\n",
       " '137',\n",
       " '137432',\n",
       " '137ppi',\n",
       " '138',\n",
       " '138026',\n",
       " '139',\n",
       " '13after',\n",
       " '13damage',\n",
       " '13gb',\n",
       " '13gbs',\n",
       " '13i',\n",
       " '13just',\n",
       " '13m',\n",
       " '13mb',\n",
       " '13mega',\n",
       " '13megapix',\n",
       " '13megapixal',\n",
       " '13megapixel',\n",
       " '13mp',\n",
       " '13mpix',\n",
       " '13mpx',\n",
       " '13now',\n",
       " '13okay',\n",
       " '13still',\n",
       " '13th',\n",
       " '13the',\n",
       " '13yo',\n",
       " '13yr',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '1400mah',\n",
       " '140lbs',\n",
       " '140mb',\n",
       " '140x176',\n",
       " '1410',\n",
       " '1414',\n",
       " '1417401743',\n",
       " '142',\n",
       " '14234',\n",
       " '1425066960',\n",
       " '1425067051',\n",
       " '1429280147',\n",
       " '143',\n",
       " '1437',\n",
       " '14393',\n",
       " '143f',\n",
       " '144',\n",
       " '1440',\n",
       " '1440p',\n",
       " '1440x2560',\n",
       " '1441193091',\n",
       " '144gb',\n",
       " '145',\n",
       " '1450mah',\n",
       " '1457',\n",
       " '146',\n",
       " '1464776396',\n",
       " '1469458904',\n",
       " '1478550065',\n",
       " '148',\n",
       " '149',\n",
       " '14am',\n",
       " '14bought',\n",
       " '14days',\n",
       " '14for',\n",
       " '14gb',\n",
       " '14hr',\n",
       " '14hrs',\n",
       " '14i',\n",
       " '14mbps',\n",
       " '14mm',\n",
       " '14mp',\n",
       " '14nm',\n",
       " '14phone',\n",
       " '14th',\n",
       " '14x8x7',\n",
       " '14yr',\n",
       " '14yrs',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '1500ma',\n",
       " '1500mah',\n",
       " '1500mhz',\n",
       " '15015',\n",
       " '150ftlb',\n",
       " '150it',\n",
       " '150kb',\n",
       " '150mah',\n",
       " '150mb',\n",
       " '150mbps',\n",
       " '150s60v3',\n",
       " '151',\n",
       " '152',\n",
       " '1520',\n",
       " '153',\n",
       " '1533',\n",
       " '1535',\n",
       " '1536x2048',\n",
       " '154',\n",
       " '1545',\n",
       " '155',\n",
       " '156',\n",
       " '158',\n",
       " '159',\n",
       " '15after',\n",
       " '15at',\n",
       " '15days',\n",
       " '15ended',\n",
       " '15f',\n",
       " '15ft',\n",
       " '15gb',\n",
       " '15gbp',\n",
       " '15had',\n",
       " '15i',\n",
       " '15mbps',\n",
       " '15min',\n",
       " '15mins',\n",
       " '15minutes',\n",
       " '15mis',\n",
       " '15mm',\n",
       " '15mph',\n",
       " '15percent',\n",
       " '15phone',\n",
       " '15pm',\n",
       " '15s',\n",
       " '15so',\n",
       " '15th',\n",
       " '15up',\n",
       " '15updated',\n",
       " '15was',\n",
       " '15yo',\n",
       " '15yr',\n",
       " '15yrs',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '1600x1200',\n",
       " '160412',\n",
       " '160gb',\n",
       " '160x120pxphoto',\n",
       " '161keypad',\n",
       " '162',\n",
       " '1620',\n",
       " '163',\n",
       " '1630',\n",
       " '163972',\n",
       " '164',\n",
       " '1640',\n",
       " '165',\n",
       " '1650',\n",
       " '1650ma',\n",
       " '1650mah',\n",
       " '166',\n",
       " '168',\n",
       " '168th',\n",
       " '169',\n",
       " '16after',\n",
       " '16bg',\n",
       " '16bought',\n",
       " '16dropped',\n",
       " '16fps',\n",
       " '16g',\n",
       " '16gb',\n",
       " '16gb3g',\n",
       " '16gbnow',\n",
       " '16gbpexcellent',\n",
       " '16gbs',\n",
       " '16gig',\n",
       " '16gigs',\n",
       " '16great',\n",
       " '16hrs',\n",
       " '16i',\n",
       " '16just',\n",
       " '16last',\n",
       " '16m',\n",
       " '16mate',\n",
       " '16mb',\n",
       " '16mm',\n",
       " '16mp',\n",
       " '16mplong',\n",
       " '16nm',\n",
       " '16so',\n",
       " '16th',\n",
       " '16the',\n",
       " '16this',\n",
       " '16using',\n",
       " '16well',\n",
       " '16x',\n",
       " '16x9',\n",
       " '16year',\n",
       " '16yesterday',\n",
       " '17',\n",
       " '170',\n",
       " '1700',\n",
       " '1700_2100',\n",
       " '1700mhz',\n",
       " '170bucks',\n",
       " '171',\n",
       " '1717',\n",
       " '171730876692',\n",
       " '172',\n",
       " '1727',\n",
       " '173',\n",
       " '174',\n",
       " '1747',\n",
       " '175',\n",
       " '176',\n",
       " '1762',\n",
       " '177',\n",
       " '17787635',\n",
       " '1779',\n",
       " '179',\n",
       " '17days',\n",
       " '17gb',\n",
       " '17mb',\n",
       " '17min',\n",
       " '17mm',\n",
       " '17th',\n",
       " '17they',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '18002',\n",
       " '1800588889i',\n",
       " '1800mah',\n",
       " '1800mhz',\n",
       " '180can',\n",
       " '180degrees',\n",
       " '181',\n",
       " '181632986587',\n",
       " '181mb',\n",
       " '182',\n",
       " '183',\n",
       " '1835',\n",
       " '184',\n",
       " '1840',\n",
       " '185',\n",
       " '185g',\n",
       " '186',\n",
       " '187',\n",
       " '1871233',\n",
       " '188',\n",
       " '188mb',\n",
       " '189',\n",
       " '18gb',\n",
       " '18h',\n",
       " '18hrs',\n",
       " '18mb',\n",
       " '18mp',\n",
       " '18th',\n",
       " '18x18',\n",
       " '19',\n",
       " '190',\n",
       " '1900',\n",
       " '19003g',\n",
       " '1900ghz',\n",
       " '1900mah',\n",
       " '1900mhz',\n",
       " '1900mhzdata',\n",
       " '1900mhzdigitel',\n",
       " '1900mhzmovilnet',\n",
       " '1900mhzresult',\n",
       " '1900red',\n",
       " '1900wcdma',\n",
       " '1905',\n",
       " '191',\n",
       " '192',\n",
       " '1920',\n",
       " '1920x1080',\n",
       " '1920x1080p',\n",
       " '1920x1200',\n",
       " '19220',\n",
       " '193',\n",
       " '193was',\n",
       " '1940',\n",
       " '1940s',\n",
       " '1946087',\n",
       " '195',\n",
       " '19505',\n",
       " '1952',\n",
       " '196',\n",
       " '1960',\n",
       " '1965',\n",
       " '1968',\n",
       " '1969',\n",
       " '196ppi',\n",
       " '197',\n",
       " '1970',\n",
       " '1972',\n",
       " '197328640',\n",
       " '1974',\n",
       " '1978',\n",
       " '1979i',\n",
       " '198',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1983',\n",
       " '1984',\n",
       " '1988',\n",
       " '1989',\n",
       " '199',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1992',\n",
       " '1993',\n",
       " '1995',\n",
       " '1996',\n",
       " '1998',\n",
       " '1999',\n",
       " '199but',\n",
       " '19down',\n",
       " '19th',\n",
       " '19thth',\n",
       " '1_10',\n",
       " '1_st_s960_baralajbithis',\n",
       " '1a',\n",
       " '1adac',\n",
       " '1algunos',\n",
       " '1android',\n",
       " '1b',\n",
       " '1bar',\n",
       " '1big',\n",
       " '1bn',\n",
       " '1build',\n",
       " '1c',\n",
       " '1camera',\n",
       " '1cents',\n",
       " '1ch',\n",
       " '1cm',\n",
       " '1cons',\n",
       " '1cpu',\n",
       " '1d',\n",
       " '1day',\n",
       " '1era',\n",
       " '1ero',\n",
       " '1ft',\n",
       " '1g',\n",
       " '1gb',\n",
       " '1gb4',\n",
       " '1gbcpu',\n",
       " '1gh',\n",
       " '1ghz',\n",
       " '1gig',\n",
       " '1h',\n",
       " '1hoir',\n",
       " '1hour',\n",
       " '1hr',\n",
       " '1i',\n",
       " '1inch',\n",
       " '1it',\n",
       " '1just',\n",
       " '1k',\n",
       " '1kb',\n",
       " '1km',\n",
       " '1lollipop',\n",
       " '1m',\n",
       " '1mb',\n",
       " '1mbps',\n",
       " '1min',\n",
       " '1minute',\n",
       " '1mm',\n",
       " '1mo',\n",
       " '1mobile',\n",
       " '1month',\n",
       " '1mp',\n",
       " '1mt',\n",
       " '1my',\n",
       " '1nce',\n",
       " '1of',\n",
       " '1official',\n",
       " '1once',\n",
       " '1oo',\n",
       " '1packet',\n",
       " '1password',\n",
       " '1pm',\n",
       " '1power',\n",
       " '1quad',\n",
       " '1resolution',\n",
       " '1rs',\n",
       " '1rst',\n",
       " '1s',\n",
       " '1service',\n",
       " '1sim',\n",
       " '1simultaneously',\n",
       " '1st',\n",
       " '1star',\n",
       " '1the',\n",
       " '1this',\n",
       " '1to',\n",
       " '1updateafter',\n",
       " '1v',\n",
       " '1we',\n",
       " '1week',\n",
       " '1who',\n",
       " '1with',\n",
       " '1x',\n",
       " '1y',\n",
       " '1year',\n",
       " '1yr',\n",
       " '1ze13a030280162940best',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2000ma',\n",
       " '2000mah',\n",
       " '2000s',\n",
       " '2001',\n",
       " '2002',\n",
       " '2002my',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2005if',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2008i',\n",
       " '2008ish',\n",
       " '2009',\n",
       " '20099',\n",
       " '2009well',\n",
       " '200buks',\n",
       " '200g',\n",
       " '200gb',\n",
       " '200gbbrowsing',\n",
       " '200great',\n",
       " '200ish',\n",
       " '200kbs',\n",
       " '200looks',\n",
       " '200mb',\n",
       " '200mbps',\n",
       " '200meg',\n",
       " '200min',\n",
       " '200smooth',\n",
       " '200sqft',\n",
       " '200the',\n",
       " '200this',\n",
       " '200usd',\n",
       " '201',\n",
       " '2010',\n",
       " '2010ish',\n",
       " '2011',\n",
       " '2011a',\n",
       " '2011and',\n",
       " '2011i',\n",
       " '2011right',\n",
       " '2012',\n",
       " '2012another',\n",
       " '2012dial',\n",
       " '2012i',\n",
       " '2012ish',\n",
       " '2012it',\n",
       " '2012symbian',\n",
       " '2013',\n",
       " '2013a',\n",
       " '2013have',\n",
       " '2013hello',\n",
       " '2013i',\n",
       " '2013it',\n",
       " '2013my',\n",
       " '2013new',\n",
       " '2013ok',\n",
       " '2013phone',\n",
       " '2013received',\n",
       " '2013still',\n",
       " '2013t',\n",
       " '2013update',\n",
       " '2013well',\n",
       " '2014',\n",
       " '20140825134330',\n",
       " '2014after',\n",
       " '2014an',\n",
       " '2014as',\n",
       " '2014beeped',\n",
       " '2014have',\n",
       " '2014i',\n",
       " '2014it',\n",
       " '2014the',\n",
       " '2014well',\n",
       " '2014øhoward',\n",
       " '2015',\n",
       " '201505',\n",
       " '20150511132052841',\n",
       " '2015after',\n",
       " '2015as',\n",
       " '2015battery',\n",
       " '2015don',\n",
       " '2015i',\n",
       " '2015it',\n",
       " '2015my',\n",
       " '2015okay',\n",
       " '2015order',\n",
       " '2015phone',\n",
       " '2015straight',\n",
       " '2015the',\n",
       " '2015track',\n",
       " '2016',\n",
       " '20161',\n",
       " '2016after',\n",
       " '2016anker',\n",
       " '2016can',\n",
       " '2016changed',\n",
       " '2016edit',\n",
       " '2016first',\n",
       " '2016for',\n",
       " '2016heard',\n",
       " '2016i',\n",
       " '2016just',\n",
       " '2016my',\n",
       " '2016ok',\n",
       " '2016one',\n",
       " '2016rating',\n",
       " '2016received',\n",
       " '2016regarding',\n",
       " '2016removable',\n",
       " '2016samsung',\n",
       " '2016sent',\n",
       " '2016so',\n",
       " '2016stamina',\n",
       " '2016the',\n",
       " '2016this',\n",
       " '2016tl',\n",
       " '2016to',\n",
       " '2016tried',\n",
       " '2016was',\n",
       " '2016well',\n",
       " '2017',\n",
       " '2018',\n",
       " '202',\n",
       " '2020',\n",
       " '2025',\n",
       " '203',\n",
       " '2039',\n",
       " '204',\n",
       " '2043',\n",
       " '2048those',\n",
       " '2048x1536',\n",
       " '205',\n",
       " '2050',\n",
       " '2050mah',\n",
       " '206',\n",
       " '2060',\n",
       " '207',\n",
       " '2070',\n",
       " '20781',\n",
       " '207r',\n",
       " '209',\n",
       " '20also',\n",
       " '20am',\n",
       " '20fps',\n",
       " '20ft',\n",
       " '20g',\n",
       " '20gb',\n",
       " '20hours',\n",
       " '20hrs',\n",
       " '20ish',\n",
       " '20mbps',\n",
       " '20mbs',\n",
       " '20min',\n",
       " '20mins',\n",
       " '20minutes',\n",
       " '20mis',\n",
       " '20mp',\n",
       " '20pm',\n",
       " '20received',\n",
       " '20s',\n",
       " '20th',\n",
       " '20us',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-df9b680efe8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda5\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3081\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression classification\n",
    "\n",
    "We will train a logistic regression model to classify the  Amazon reviews into positive and negative reviews by using feature matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(vect2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"No it did not, you can't hear when you have it on speaker phone\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-bb1b6856bada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Predict the transformed test documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AUC: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda5\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda5\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    298\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda5\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                       force_all_finite)\n\u001b[0;32m    401\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: \"No it did not, you can't hear when you have it on speaker phone\""
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict((X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.51217593, -0.12377712,  0.06281552, ...,  0.34869348,\n",
       "         0.257767  ,  0.257767  ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52365, 26658, 52373, ..., 18341, 18306, 18305], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_[0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 52365 is out of bounds for axis 1 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-84206b18f1c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Find the 10 smallest and 10 largest coefficients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Smallest Coefs:'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msorted_coef_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n Largest Coefs:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 52365 is out of bounds for axis 1 with size 7"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect1.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "print('Smallest Coefs:' )\n",
    "print(feature_names[sorted_coef_index[:10]])\n",
    "      \n",
    "print('\\n Largest Coefs:')      \n",
    "print(feature_names[sorted_coef_index[:-11:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf\n",
    "\n",
    "When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don't contain useful or discriminatory information. In this subsection, we will learn about a useful technique called **term frequency-inverse document frequency** (*tf-idf*) that can be used to downweight those frequently occurring words in the feature vectors. On the other words by tf-idf we can reduce the weightage of more common words like (the, is, an etc.) which occurs in all document.\n",
    "\n",
    "The *tf-idf* can be defined as the product of the term frequency and the inverse document frequency:\n",
    "\n",
    "\\begin{align}\n",
    "\\textit{tf-idf}(t,d) = tf(t,d) \\times idf(t,d)\n",
    "\\end{align}\n",
    "\n",
    "Here the *tf(t,d)* is the term frequency that equal to Count of word / Total words, in each document. The inverse document frequency *idf(t,d)* can be calculated as:\n",
    "\n",
    "\\begin{align}\n",
    "idf(t,d) = log\\frac{n_d}{1+\\text{df(d,t)}}\n",
    "\\end{align}\n",
    "\n",
    "where $n_d$ is the total number of documents, and *df(d,t)* is the number of documents *d* that contain the term *t*. Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in all training samples; the log is used to ensure that low document frequencies are not given too much weight.\n",
    "\n",
    "\n",
    "scikit-learn implements yet another vectorizer, the TfidfVectorizer, that creates feature vectors as tf-idfs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.43370786,  0.55847784,  0.55847784,  0.        ,\n",
       "         0.43370786,  0.        ],\n",
       "       [ 0.        ,  0.43370786,  0.        ,  0.        ,  0.55847784,\n",
       "         0.43370786,  0.55847784],\n",
       "       [ 0.40474829,  0.47810172,  0.30782151,  0.30782151,  0.30782151,\n",
       "         0.47810172,  0.30782151]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'])\n",
    "\n",
    "vect2 = TfidfVectorizer().fit(docs)\n",
    "bag2 = vect2.transform(docs)\n",
    "bag2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data \n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18025"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names()) #notice our feature is reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.926957067251\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Exercise\n",
    " \n",
    "- Predict two below reviews as negetive or positive using our model: \n",
    "\n",
    "      ['not an issue, phone is working', 'an issue, phone is not working']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829,  0.40824829,  0.40824829,  0.40824829,  0.40824829,\n",
       "         0.40824829],\n",
       "       [ 0.40824829,  0.40824829,  0.40824829,  0.40824829,  0.40824829,\n",
       "         0.40824829]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "docs = np.array([\n",
    "    'not an issue, phone is working', \n",
    "    'an issue, phone is not working'])\n",
    "\n",
    "vect3 = TfidfVectorizer().fit(docs)\n",
    "bag3 = vect3.transform(docs)\n",
    "bag3.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-929e6dd7dafe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'not an issue, phone is working'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'an issue, phone is not working'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "predictions = model.predict(vect.transform['not an issue, phone is working', 'an issue, phone is not working'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams\n",
    "\n",
    "The sequence of items in the bag-of-words model that we just created is also called the 1-gram or unigram model — each item or token in the vocabulary represents a single word. Generally, <b>the contiguous sequences of items in NLP</b> — words, letters, or symbols— is also called an n-gram. The choice of the number n in the n-gram model depends on the particular application. For instance, spam filtering applications tend to use n=3 or n=4 for good performances.\n",
    "To summarize the concept of the n-gram representation, the 1-gram and 2-gram representations of our first document \"the sun is shining\" would be constructed as follows:\n",
    "- 1-gram: \"the\", \"sun\", \"is\", \"shining\"\n",
    "- 2-gram: \"the sun\", \"sun is\", \"is shining\"\n",
    "\n",
    "The CountVectorizer class in scikit-learn allows us to use different n-gram models via its ngram_range parameter. By default, it uses a 1-gram representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try 2-gram representation\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'])\n",
    "\n",
    "vect3=CountVectorizer(ngram_range=(2,2)).fit(docs)\n",
    "bag3=vect3.transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train) #use both1 gram and bi-gram\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201035"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.963469711789\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['no good' 'junk' 'worst' 'horrible' 'not good' 'garbage' 'not happy'\n",
      " 'terrible' 'not satisfied' 'not very']\n",
      "\n",
      " Largest Coefs:\n",
      "['excelent' 'excelente' 'not bad' 'excellent' 'perfect' 'no problems'\n",
      " 'awesome' 'exelente' 'great' 'no issues']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:' )\n",
    "print(feature_names[sorted_coef_index[:10]])\n",
    "      \n",
    "print('\\n Largest Coefs:')      \n",
    "print(feature_names[sorted_coef_index[:-11:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "# These reviews are now correctly identified\n",
    "print(model.predict(vect.transform(['no issues, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "## Using sklearn's NaiveBayes Classifier\n",
    "\n",
    "\n",
    "### Exercise:\n",
    "1. Do text classification for the Amazon reviews dataset using NaiveBayes Classifier\n",
    "2. Evaluate your classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the TfidfVectorizer to the training data \n",
    "X_train_vectorized = TfidfVectorizer(min_df=5).fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.787527653554\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.937650330091\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95108343064746337"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test,predictions,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95108343064746337"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95108343064746337"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_test,predictions,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
